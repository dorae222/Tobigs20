{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede8b49b",
   "metadata": {},
   "source": [
    "# 목차\n",
    "\n",
    "0. Preprocessing\n",
    "1. Bagging\n",
    "    - 1.a 직접 구현\n",
    "        - [참고논문: Bagging Predictors](https://www.stat.berkeley.edu/~breiman/bagging.pdf)\n",
    "    - 1.b. 라이브러리 활용\n",
    "2. Random Forest\n",
    "    - 2.a 직접 구현\n",
    "        - [참고논문: Random Forest](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n",
    "    - 2.b. 라이브러리 활용\n",
    "3. Boosting\n",
    "    - 3.1 XGBoost\n",
    "        - 3.1.a 직접 구현\n",
    "            - [참고논문: XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)\n",
    "        - 3.1.b 라이브러리 활용\n",
    "    - 3.2 LightGBM\n",
    "        - 3.2.a 직접 구현\n",
    "            - [참고논문: LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n",
    "        - 2.2.b 라이브러리 활용\n",
    "    - 3.3 CatBoost\n",
    "<!--         - 3.3.a 직접 구현\n",
    "            - [참고논문: CatBoost: Unbiased Boosting with Categorical Features](https://arxiv.org/pdf/1706.09516.pdf) -->\n",
    "        - 3.3.b 라이브러리 활용\n",
    "4. 결과 및 느낀점\n",
    "\n",
    "- *DSBA 연구실 강의와 투빅스 자료를 함께 참고하였습니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8900441",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057798ac",
   "metadata": {},
   "source": [
    "# 0. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd419405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 적재 및 전처리를 위한 라이브러리\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 직접 구현 관련 라이브러리\n",
    "import itertools\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 모델을 포함한 라이브러리\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757d6135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIENTNUM</th>\n",
       "      <th>Attrition_Flag</th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income_Category</th>\n",
       "      <th>Card_Category</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>...</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Avg_Open_To_Buy</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>Total_Trans_Amt</th>\n",
       "      <th>Total_Trans_Ct</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <th>Avg_Utilization_Ratio</th>\n",
       "      <th>Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1</th>\n",
       "      <th>Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768805383</td>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>45</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>High School</td>\n",
       "      <td>Married</td>\n",
       "      <td>$60K - $80K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>12691.0</td>\n",
       "      <td>777</td>\n",
       "      <td>11914.0</td>\n",
       "      <td>1.335</td>\n",
       "      <td>1144</td>\n",
       "      <td>42</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.99991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>818770008</td>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Single</td>\n",
       "      <td>Less than $40K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>8256.0</td>\n",
       "      <td>864</td>\n",
       "      <td>7392.0</td>\n",
       "      <td>1.541</td>\n",
       "      <td>1291</td>\n",
       "      <td>33</td>\n",
       "      <td>3.714</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.99994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>713982108</td>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Married</td>\n",
       "      <td>$80K - $120K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>2.594</td>\n",
       "      <td>1887</td>\n",
       "      <td>20</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.99998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769911858</td>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>High School</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Less than $40K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>3313.0</td>\n",
       "      <td>2517</td>\n",
       "      <td>796.0</td>\n",
       "      <td>1.405</td>\n",
       "      <td>1171</td>\n",
       "      <td>20</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.99987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>709106358</td>\n",
       "      <td>Existing Customer</td>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>Uneducated</td>\n",
       "      <td>Married</td>\n",
       "      <td>$60K - $80K</td>\n",
       "      <td>Blue</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>4716.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4716.0</td>\n",
       "      <td>2.175</td>\n",
       "      <td>816</td>\n",
       "      <td>28</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.99998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CLIENTNUM     Attrition_Flag  Customer_Age Gender  Dependent_count  \\\n",
       "0  768805383  Existing Customer            45      M                3   \n",
       "1  818770008  Existing Customer            49      F                5   \n",
       "2  713982108  Existing Customer            51      M                3   \n",
       "3  769911858  Existing Customer            40      F                4   \n",
       "4  709106358  Existing Customer            40      M                3   \n",
       "\n",
       "  Education_Level Marital_Status Income_Category Card_Category  \\\n",
       "0     High School        Married     $60K - $80K          Blue   \n",
       "1        Graduate         Single  Less than $40K          Blue   \n",
       "2        Graduate        Married    $80K - $120K          Blue   \n",
       "3     High School        Unknown  Less than $40K          Blue   \n",
       "4      Uneducated        Married     $60K - $80K          Blue   \n",
       "\n",
       "   Months_on_book  ...  Credit_Limit  Total_Revolving_Bal  Avg_Open_To_Buy  \\\n",
       "0              39  ...       12691.0                  777          11914.0   \n",
       "1              44  ...        8256.0                  864           7392.0   \n",
       "2              36  ...        3418.0                    0           3418.0   \n",
       "3              34  ...        3313.0                 2517            796.0   \n",
       "4              21  ...        4716.0                    0           4716.0   \n",
       "\n",
       "   Total_Amt_Chng_Q4_Q1  Total_Trans_Amt  Total_Trans_Ct  Total_Ct_Chng_Q4_Q1  \\\n",
       "0                 1.335             1144              42                1.625   \n",
       "1                 1.541             1291              33                3.714   \n",
       "2                 2.594             1887              20                2.333   \n",
       "3                 1.405             1171              20                2.333   \n",
       "4                 2.175              816              28                2.500   \n",
       "\n",
       "   Avg_Utilization_Ratio  \\\n",
       "0                  0.061   \n",
       "1                  0.105   \n",
       "2                  0.000   \n",
       "3                  0.760   \n",
       "4                  0.000   \n",
       "\n",
       "   Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1  \\\n",
       "0                                           0.000093                                                                                    \n",
       "1                                           0.000057                                                                                    \n",
       "2                                           0.000021                                                                                    \n",
       "3                                           0.000134                                                                                    \n",
       "4                                           0.000022                                                                                    \n",
       "\n",
       "   Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2  \n",
       "0                                            0.99991                                                                                   \n",
       "1                                            0.99994                                                                                   \n",
       "2                                            0.99998                                                                                   \n",
       "3                                            0.99987                                                                                   \n",
       "4                                            0.99998                                                                                   \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "file_path = \"./BankChurners.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 처음 5개의 행을 출력하여 데이터를 확인\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e157533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 컬럼 삭제\n",
    "data = data.drop(data.columns[-2:], axis=1)\n",
    "\n",
    "# 목표 변수 인코딩: 'Existing Customer'를 0, 'Attrited Customer'를 1로 인코딩\n",
    "target_encoder = LabelEncoder()\n",
    "data['Attrition_Flag'] = target_encoder.fit_transform(data['Attrition_Flag'])\n",
    "\n",
    "# 범주형 피처 인코딩\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "for feature in categorical_features:\n",
    "    encoder = LabelEncoder()\n",
    "    data[feature] = encoder.fit_transform(data[feature])\n",
    "\n",
    "# 데이터 분할: 학습 및 테스트 데이터셋으로 분할\n",
    "X = data.drop('Attrition_Flag', axis=1)  # 피처\n",
    "y = data['Attrition_Flag']               # 목표 변수\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e3553",
   "metadata": {},
   "source": [
    "# 1. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa15c73",
   "metadata": {},
   "source": [
    "## 1.a. 직접구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767ba4b",
   "metadata": {},
   "source": [
    "#### 특징:\n",
    "1. Bagging 방법을 사용하여 여러 개의 결정 트리를 학습합니다.\n",
    "2. 각 트리에 대해 샘플과 특징을 랜덤하게 선택하여 학습시킵니다.\n",
    "\n",
    "#### 주요 변수:\n",
    "- `n_estimators`: 학습할 결정 트리의 수\n",
    "- `max_samples`: 각 트리를 학습할 때 사용할 샘플의 비율\n",
    "- `max_features`: 각 트리를 학습할 때 사용할 특징의 비율\n",
    "- `trees`: 학습된 결정 트리와 해당 트리를 학습할 때 사용된 특징의 인덱스를 저장하는 리스트\n",
    "\n",
    "#### 주요 메서드:\n",
    "\n",
    "1. `fit(X, y)`:\n",
    "    - 입력 데이터 `X`와 레이블 `y`를 사용하여 여러 개의 결정 트리를 학습시킵니다.\n",
    "    - 각 트리를 학습할 때는 `max_samples` 비율의 샘플과 `max_features` 비율의 특징을 랜덤하게 선택하여 사용합니다.\n",
    "    - 학습된 결정 트리와 사용된 특징의 인덱스는 `trees` 리스트에 저장됩니다.\n",
    "\n",
    "2. `predict(X)`:\n",
    "    - 입력 데이터 `X`에 대한 예측을 수행합니다.\n",
    "    - 각 결정 트리에서 예측을 수행하고, 최종 예측은 모든 트리의 예측 결과를 다수결 방식으로 결정합니다.\n",
    "\n",
    "#### 동작 방식:\n",
    "1. Bagging 방법을 사용하여 여러 개의 결정 트리를 학습합니다. 각 트리는 데이터의 일부 샘플과 특징을 랜덤하게 선택하여 학습되기 때문에, 각 트리는 서로 다른 특성을 가집니다.\n",
    "2. 예측 시, 모든 트리의 예측 결과를 모아 다수결 방식으로 최종 예측을 결정합니다. 이렇게 하면 개별 트리의 예측 오차를 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287da1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingImplementation:\n",
    "    def __init__(self, n_estimators=20, max_samples=0.8, max_features=0.8):\n",
    "        self.n_estimators = n_estimators  # base learner의 개수\n",
    "        self.max_samples = max_samples  # 각 base learner에 대한 샘플링 비율\n",
    "        self.max_features = max_features  # 각 base learner에 대한 특징 선택 비율\n",
    "        self.trees = []  # base learner들을 저장할 리스트\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)  # X를 Numpy array로 변환\n",
    "        y = np.array(y)  # y를 Numpy array로 변환\n",
    "        \n",
    "        n_samples, n_features = X.shape  # 데이터의 샘플 수와 특징 수를 구함\n",
    "        \n",
    "        for _ in range(self.n_estimators):  # 지정된 개수만큼 base learner를 학습\n",
    "            # 샘플과 특징을 랜덤하게 선택\n",
    "            sample_idxs = np.random.choice(n_samples, int(n_samples * self.max_samples), replace=True)\n",
    "            feature_idxs = np.random.choice(n_features, int(n_features * self.max_features), replace=False)\n",
    "            \n",
    "            # 선택된 샘플과 특징으로 데이터를 구성\n",
    "            X_subset = X[sample_idxs][:, feature_idxs]\n",
    "            y_subset = y[sample_idxs]\n",
    "            \n",
    "            # 결정 트리를 학습 (다른 알고리즘으로도 대체 가능)\n",
    "            # 단, 복잡도가 높은 모델이 높은 성능 기대\n",
    "            tree = DecisionTreeClassifier()\n",
    "            tree.fit(X_subset, y_subset)\n",
    "            \n",
    "            # 학습된 tree와 사용된 특징의 인덱스를 저장\n",
    "            self.trees.append((tree, feature_idxs))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # 각 base learner의 예측을 수집\n",
    "        X = np.array(X)  # X를 Numpy array로 변환\n",
    "        predictions = []\n",
    "        for tree, feature_idxs in self.trees:\n",
    "            X_subset = X[:, feature_idxs]  # 사용된 특징만 선택\n",
    "            predictions.append(tree.predict(X_subset))  # 예측 수행\n",
    "            \n",
    "        # 수집된 예측을 통해 최종 예측을 결정 (다수결 방식)\n",
    "        predictions = np.array(predictions)\n",
    "        final_predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            final_predictions.append(np.bincount(predictions[:, i]).argmax())\n",
    "        \n",
    "        return np.array(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a2d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "bagging_implemantation = BaggingImplementation()\n",
    "\n",
    "start_time_train = time.time()\n",
    "bagging_implemantation.fit(X_train.values, y_train.values)\n",
    "end_time_train = time.time()\n",
    "bagging_implemantation_training_time = end_time_train - start_time_train\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "bagging_implemantation_predictions = bagging_implemantation.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "bagging_implemantation_accuracy = accuracy_score(y_test.values, bagging_implemantation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809ab681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging_implemantation_accuracy: 0.9610069101678184\n",
      "bagging_implemantation_training_time: 0.5716240406036377\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('bagging_implemantation_accuracy:',bagging_implemantation_accuracy)\n",
    "print('bagging_implemantation_training_time:',bagging_implemantation_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96377fb",
   "metadata": {},
   "source": [
    "## 1.b. 라이브러리 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196220c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base estimator로 결정 트리를 사용\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Bagging 분류기 생성\n",
    "bagging_clf = BaggingClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "\n",
    "\n",
    "# 모델 학습\n",
    "start_time_train_bagging = time.time()  # 학습 시작 시간 저장\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "end_time_train_bagging = time.time()  # 학습 종료 시간 저장\n",
    "scikitlearn_bagging_training_time= end_time_train_bagging - start_time_train_bagging\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "scikitlearn_bagging_predictions =bagging_clf.predict(X_test)\n",
    "\n",
    "# 분류 정확도 계산\n",
    "scikitlearn_bagging_accuracy = accuracy_score(y_test, scikitlearn_bagging_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54384c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikitlearn_bagging_accuracy: 0.9580454096742349\n",
      "scikitlearn_bagging_training_time: 1.455275058746338\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('scikitlearn_bagging_accuracy:',scikitlearn_bagging_accuracy)\n",
    "print('scikitlearn_bagging_training_time:',scikitlearn_bagging_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3210251",
   "metadata": {},
   "source": [
    "# 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821ec90",
   "metadata": {},
   "source": [
    "## 2.a. 직접 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a72d5c",
   "metadata": {},
   "source": [
    "#### 특징:\n",
    "1. 여러 개의 결정 트리를 학습하여 앙상블 방식으로 분류 작업을 수행합니다.\n",
    "2. 각 트리는 부트스트랩 샘플링 방법으로 생성된 서브셋에서 학습됩니다.\n",
    "3. 예측 시, 모든 트리의 예측 결과를 모아 다수결 방식으로 최종 예측을 결정합니다.\n",
    "\n",
    "#### 주요 변수:\n",
    "- `n_estimators`: 학습할 결정 트리의 수\n",
    "- `max_depth`: 각 트리의 최대 깊이\n",
    "- `max_features`: 각 트리를 학습할 때 선택될 특징의 최대 수 (예: \"sqrt\"는 전체 특징 수의 제곱근만큼의 특징을 선택)\n",
    "- `min_samples_split`: 노드를 분할하기 위한 최소한의 샘플 수\n",
    "- `trees`: 학습된 결정 트리들을 저장하는 리스트\n",
    "\n",
    "#### 메서드:\n",
    "\n",
    "1. `fit(X, y)`:\n",
    "    - 입력 데이터 `X`와 레이블 `y`를 사용하여 여러 개의 결정 트리를 학습시킵니다.\n",
    "    - 각 트리는 부트스트랩 샘플링 방법으로 생성된 데이터 서브셋에서 학습됩니다.\n",
    "    - 학습된 결정 트리는 `trees` 리스트에 저장됩니다.\n",
    "\n",
    "2. `predict(X)`:\n",
    "    - 입력 데이터 `X`에 대한 예측을 수행합니다.\n",
    "    - 각 결정 트리에서 예측을 수행하고, 최종 예측은 모든 트리의 예측 결과를 모아 다수결 방식으로 결정합니다.\n",
    "\n",
    "#### 동작 방식:\n",
    "1. 랜덤 포레스트는 앙상블 방식으로 분류 작업을 수행하는 데 여러 개의 결정 트리를 사용합니다. 각 트리는 데이터의 부트스트랩 샘플에서 학습되므로, 각 트리는 서로 다른 특성을 가집니다.\n",
    "2. 예측 시, 모든 트리의 예측 결과를 모아서 다수결 방식으로 최종 예측을 결정합니다. 이렇게 하면 개별 트리의 예측 오차를 줄일 수 있으며, 전체적인 예측 성능이 향상됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3399853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestImplementation:\n",
    "    def __init__(self, n_estimators=20, max_depth=None, max_features='sqrt', min_samples_split=2):\n",
    "        self.n_estimators = n_estimators  # 트리의 개수\n",
    "        self.max_depth = max_depth  # 트리의 최대 깊이\n",
    "        self.max_features = max_features  # 각 노드에서 선택할 특성의 개수\n",
    "        self.min_samples_split = min_samples_split  # 노드를 분할하기 위한 최소 샘플 수\n",
    "        self.trees = []  # 학습된 트리들을 저장할 리스트\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)  # X를 Numpy array로 변환\n",
    "        y = np.array(y)  # y를 Numpy array로 변환\n",
    "        \n",
    "        n_samples, n_features = X.shape  # 데이터의 샘플 수와 특징 수를 구함\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # 부트스트랩 샘플 생성\n",
    "            sample_idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_bootstrap = X[sample_idxs]\n",
    "            y_bootstrap = y[sample_idxs]\n",
    "            \n",
    "            # 결정 트리 생성 및 학습\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, \n",
    "                                          max_features=self.max_features, \n",
    "                                          min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            \n",
    "            # 학습된 트리 저장\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)  # X를 Numpy array로 변환\n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        # 각 트리의 예측을 수집\n",
    "        for tree in self.trees:\n",
    "            predictions += tree.predict(X)\n",
    "        \n",
    "        # 수집된 예측을 통해 최종 예측을 결정 (다수결 방식)\n",
    "        final_predictions = (predictions / self.n_estimators >= 0.5).astype(int)\n",
    "        \n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7fd02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "RF_implemantation = RandomForestImplementation()\n",
    "\n",
    "start_time_train = time.time()\n",
    "RF_implemantation.fit(X_train.values, y_train.values)\n",
    "end_time_train = time.time()\n",
    "RF_implemantation_training_time = end_time_train - start_time_train\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "RF_implemantation_predictions = RF_implemantation.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "RF_implemantation_accuracy = accuracy_score(y_test.values, RF_implemantation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6437bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_implemantation_accuracy: 0.9580454096742349\n",
      "RF_implemantation_training_time: 0.2165968418121338\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('RF_implemantation_accuracy:',RF_implemantation_accuracy)\n",
    "print('RF_implemantation_training_time:',RF_implemantation_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246be577",
   "metadata": {},
   "source": [
    "## 2.b. 라이브러리 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3a61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "start_time_train = time.time()\n",
    "random_forest_clf.fit(X_train, y_train)\n",
    "end_time_train = time.time()\n",
    "training_time = end_time_train - start_time_train\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "y_pred = random_forest_clf.predict(X_test)\n",
    "\n",
    "# 분류 정확도 계산\n",
    "scikitlearn_RF_accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ba325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikitlearn_RF_accuracy: 0.9619940769990128\n",
      "scikitlearn_RF_training_time: 0.7276842594146729\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('scikitlearn_RF_accuracy:',scikitlearn_RF_accuracy)\n",
    "print('scikitlearn_RF_training_time:',training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c69cc6",
   "metadata": {},
   "source": [
    "# 3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcc2cf",
   "metadata": {},
   "source": [
    "## 3.1. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0c227",
   "metadata": {},
   "source": [
    "### 3-1-a. 직접 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d3ef2",
   "metadata": {},
   "source": [
    "#### 특징:\n",
    "1. 부스팅 방법을 사용하여 여러 개의 결정 트리를 연속적으로 학습합니다.\n",
    "2. 각 트리는 이전 트리의 예측 오차를 바탕으로 학습되며, 이를 통해 예측 성능을 향상시킵니다.\n",
    "3. L2 정규화를 사용하여 모델의 복잡도를 제한하고 과적합을 방지합니다.\n",
    "\n",
    "#### 주요 변수:\n",
    "- `n_estimators`: 학습할 결정 트리의 수\n",
    "- `learning_rate`: 각 트리의 기여도를 조절하기 위한 학습률\n",
    "- `max_depth`: 각 트리의 최대 깊이\n",
    "- `min_size`: 리프 노드가 가져야 할 최소한의 샘플 수\n",
    "- `subsample`: 각 트리를 학습할 때 사용할 샘플의 비율\n",
    "- `colsample`: 각 트리를 학습할 때 사용할 특징의 비율\n",
    "- `trees`: 학습된 결정 트리들을 저장하는 리스트\n",
    "- `lambda_reg`: L2 정규화 항\n",
    "\n",
    "#### 메서드:\n",
    "\n",
    "1. `fit(X, y)`:\n",
    "    - 입력 데이터 `X`와 레이블 `y`를 사용하여 여러 개의 결정 트리를 학습시킵니다.\n",
    "    - 각 트리는 이전 트리의 예측 오차를 바탕으로 학습되며, `subsample`과 `colsample`을 통해 데이터의 일부 샘플과 특징을 랜덤하게 선택하여 사용합니다.\n",
    "    - 학습된 결정 트리는 `trees` 리스트에 저장됩니다.\n",
    "\n",
    "2. `predict(X)`:\n",
    "    - 입력 데이터 `X`에 대한 예측을 수행합니다.\n",
    "    - 각 결정 트리의 예측값을 모두 합산하여 최종 예측을 얻습니다.\n",
    "\n",
    "3. `split_dataset(X, y, feature_index, threshold)`:\n",
    "    - 주어진 특징과 임계값을 기준으로 데이터셋을 분할합니다.\n",
    "    - 왼쪽 및 오른쪽 자식 노드의 데이터를 반환합니다.\n",
    "\n",
    "4. `calculate_gini(y)`:\n",
    "    - 주어진 레이블 데이터에 대한 지니 불순도를 계산합니다.\n",
    "\n",
    "5. `gini_with_regularization(y, num_samples)`:\n",
    "    - 주어진 레이블 데이터에 대한 지니 불순도를 계산하되, L2 정규화 항을 추가하여 반환합니다.\n",
    "\n",
    "6. `get_best_split(X, y)`:\n",
    "    - 주어진 데이터를 기반으로 최적의 분할 특징과 임계값을 찾아 반환합니다.\n",
    "\n",
    "7. `build_decision_tree(X, y, current_depth=0)`:\n",
    "    - 주어진 데이터를 기반으로 결정 트리를 재귀적으로 구성합니다.\n",
    "    - 최대 깊이나 최소 샘플 크기에 도달하면 종료합니다.\n",
    "\n",
    "8. `predict_with_tree(tree, x)`:\n",
    "    - 주어진 결정 트리를 사용하여 입력 데이터 포인트에 대한 예측값을 계산합니다.\n",
    "\n",
    "9. `cross_validate(X, y, k=5)`:\n",
    "    - k-fold 교차 검증을 수행하여 모델의 정확도를 평가합니다.\n",
    "\n",
    "10. `parallel_tree_building(data)`:\n",
    "    - 멀티프로세싱을 사용하여 병렬로 트리를 구성합니다.\n",
    "\n",
    "#### 동작 방식:\n",
    "1. XGBoost는 부스팅 방법을 사용하여 여러 개의 결정 트리를 연속적으로 학습시킵니다. 각 트리는 이전 트리들의 예측 오차를 바탕으로 학습되기 때문에, 연속적인 학습을 통해 예측 성능이 향상됩니다.\n",
    "2. 각 트리의 학습 시, 데이터의 일부 샘플과 특징을 랜덤하게 선택하여 학습되며, 이를 통해 모델의 일반화 성능이 향상됩니다.\n",
    "3. L2 정규화를 사용하여 각 트리의 복잡도를 제한하고, 과적합을 방지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225e559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost_Implemantation:\n",
    "    def __init__(self, n_estimators=20, learning_rate=0.1, max_depth=3, min_size=5, subsample=0.8, colsample=0.8, lambda_reg=1):\n",
    "        # 초기화 함수에서 모델의 하이퍼파라미터를 설정합니다.\n",
    "        \n",
    "        self.n_estimators = n_estimators  # 부스팅에서 사용될 트리의 개수\n",
    "        self.learning_rate = learning_rate  # 각 트리의 기여도를 조절\n",
    "        self.max_depth = max_depth  # 각 트리의 최대 깊이\n",
    "        self.min_size = min_size  # 리프 노드가 가져야 할 최소한의 샘플 수\n",
    "        self.subsample = subsample  # 샘플링 비율\n",
    "        self.colsample = colsample  # 피처 샘플링 비율\n",
    "        self.trees = []  # 학습된 트리들을 저장할 리스트\n",
    "        self.lambda_reg = lambda_reg  # L2 정규화를 위한 값\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 모델 학습 함수\n",
    "        \n",
    "        # 초기 예측값을 설정. 일반적으로 타겟의 평균값으로 시작\n",
    "        self.prediction = np.full(y.shape, np.mean(y))\n",
    "        \n",
    "        # 병렬 처리를 위해 멀티프로세싱을 사용하여 각 트리를 독립적으로 구축\n",
    "        pool = Pool(processes=cpu_count())\n",
    "        residuals = [y - self.prediction for _ in range(self.n_estimators)]\n",
    "        results = pool.map(self.parallel_tree_building, [(X, res) for res in residuals])\n",
    "        pool.close()\n",
    "        self.trees.extend(results)\n",
    "        \n",
    "        # 각 트리로부터 얻은 예측값을 기존의 예측값에 더함\n",
    "        for tree in results:\n",
    "            tree_predictions = np.array([self.predict_with_tree(tree, x) for x in X])\n",
    "            self.prediction += self.learning_rate * tree_predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 학습된 모델로부터 예측값을 얻는 함수\n",
    "        \n",
    "        # 초기 예측값 설정\n",
    "        final_predictions = np.full(X.shape[0], np.mean(self.prediction))\n",
    "        \n",
    "        # 각 트리로부터 얻은 예측값을 누적하여 최종 예측값을 구함\n",
    "        for tree in self.trees:\n",
    "            tree_predictions = np.array([self.predict_with_tree(tree, x) for x in X])\n",
    "            final_predictions += self.learning_rate * tree_predictions\n",
    "            \n",
    "        # 분류 문제의 경우, 임계치를 기준으로 예측 클래스를 결정\n",
    "        return (final_predictions > 0.5).astype(int)\n",
    "\n",
    "    def split_dataset(self, X, y, feature_index, threshold):\n",
    "        # 주어진 피처와 임계값을 기준으로 데이터셋을 분할하는 함수\n",
    "        \n",
    "        # 누락된 데이터 처리\n",
    "        if np.isnan(threshold):\n",
    "            left_mask = np.isnan(X[:, feature_index])\n",
    "            right_mask = ~left_mask\n",
    "        else:\n",
    "            left_mask = X[:, feature_index] < threshold\n",
    "            right_mask = ~left_mask\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def calculate_gini(self, y):\n",
    "        # 주어진 데이터의 지니 불순도를 계산하는 함수\n",
    "        \n",
    "        classes = np.unique(y)\n",
    "        gini = 1.0\n",
    "        for c in classes:\n",
    "            p = np.sum(y == c) / len(y)\n",
    "            gini -= p**2\n",
    "        return gini\n",
    "\n",
    "    def gini_with_regularization(self, y, num_samples):\n",
    "        # 정규화 항을 포함한 지니 불순도를 계산하는 함수\n",
    "        \n",
    "        gini = self.calculate_gini(y)\n",
    "        return gini + self.lambda_reg / num_samples\n",
    "\n",
    "    def get_best_split(self, X, y):\n",
    "        # 주어진 데이터에서 가장 좋은 분할을 찾는 함수\n",
    "        \n",
    "        best_gini = float('inf')\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            unique_values = np.unique(X[:, feature_index])\n",
    "            for threshold in unique_values:\n",
    "                X_left, y_left, X_right, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                left_gini = self.gini_with_regularization(y_left, len(X_left))\n",
    "                right_gini = self.gini_with_regularization(y_right, len(X_right))\n",
    "                gini = (len(y_left) * left_gini + len(y_right) * right_gini) / len(y)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def build_decision_tree(self, X, y, current_depth=0):\n",
    "        # 재귀적으로 결정 트리를 구성하는 함수\n",
    "        \n",
    "        if current_depth >= self.max_depth or len(y) <= self.min_size:\n",
    "            return {'prediction': np.mean(y)}\n",
    "        feature_index, threshold = self.get_best_split(X, y)\n",
    "        if feature_index is None:\n",
    "            return {'prediction': np.mean(y)}\n",
    "        X_left, y_left, X_right, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
    "        return {\n",
    "            'feature_index': feature_index,\n",
    "            'threshold': threshold,\n",
    "            'left': self.build_decision_tree(X_left, y_left, current_depth + 1),\n",
    "            'right': self.build_decision_tree(X_right, y_right, current_depth + 1)\n",
    "        }\n",
    "\n",
    "    def predict_with_tree(self, tree, x):\n",
    "        # 결정 트리를 사용하여 개별 데이터 포인트의 예측값을 계산하는 함수\n",
    "        \n",
    "        if 'prediction' in tree:\n",
    "            return tree['prediction']\n",
    "        feature_index = tree['feature_index']\n",
    "        threshold = tree['threshold']\n",
    "        if (np.isnan(threshold) and np.isnan(x[feature_index])) or (x[feature_index] < threshold):\n",
    "            return self.predict_with_tree(tree['left'], x)\n",
    "        else:\n",
    "            return self.predict_with_tree(tree['right'], x)\n",
    "\n",
    "    def cross_validate(self, X, y, k=5):\n",
    "        # k-fold 교차 검증을 수행하는 함수\n",
    "        \n",
    "        kf = KFold(n_splits=k)\n",
    "        scores = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            self.fit(X_train, y_train)\n",
    "            y_pred = self.predict(X_test)\n",
    "            accuracy = np.mean(y_pred == y_test)\n",
    "            scores.append(accuracy)\n",
    "        return scores\n",
    "\n",
    "    def parallel_tree_building(self, data):\n",
    "        # 병렬 처리를 위한 트리 구성 함수\n",
    "        \n",
    "        X, residuals = data\n",
    "        return self.build_decision_tree(X, residuals, current_depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f8d3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "xgb_implemantation = XGBoost_Implemantation(n_estimators=5, learning_rate=0.1, max_depth=3, min_size=5, subsample=0.8, colsample=0.8)\n",
    "\n",
    "start_time_train = time.time()\n",
    "xgb_implemantation.fit(X_train.values, y_train.values)\n",
    "end_time_train = time.time()\n",
    "xgb_implemantation_training_time = end_time_train - start_time_train\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "xgb_implemantation_predictions = xgb_implemantation.predict(X_test.values)\n",
    "\n",
    "# 정확도 계산\n",
    "xgb_implemantation_accuracy = accuracy_score(y_test.values, xgb_implemantation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5626dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('xgb_implemantation_accuracy:',xgb_implemantation_accuracy)\n",
    "print('xgb_implemantation_training_time:',xgb_implemantation_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10c930",
   "metadata": {},
   "source": [
    "### 3-1-b. 라이브러리 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f13fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "scikitlearn_xgb_model = XGBClassifier(n_estimators=5, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "start_time_train_xgb = time.time()  # 학습 시작 시간 저장\n",
    "scikitlearn_xgb_model.fit(X_train.values, y_train.values)  # 모델 학습\n",
    "end_time_train_xgb = time.time()  # 학습 종료 시간 저장\n",
    "scikitlearn_xgb_training_time= end_time_train_xgb - start_time_train_xgb\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "scikitlearn_xgb_predictions = scikitlearn_xgb_model.predict(X_test.values)\n",
    "\n",
    "# 분류 정확도 계산\n",
    "scikitlearn_xgb_accuracy = accuracy_score(y_test.values, scikitlearn_xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e56faa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikitlearn_xgb_accuracy: 0.9195459032576505\n",
      "scikitlearn_xgb_training_time: 0.030332565307617188\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('scikitlearn_xgb_accuracy:',scikitlearn_xgb_accuracy)\n",
    "print('scikitlearn_xgb_training_time:',scikitlearn_xgb_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6551f",
   "metadata": {},
   "source": [
    "## 3-2. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac29d77",
   "metadata": {},
   "source": [
    "### 3-2-a. 직접 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ae450",
   "metadata": {},
   "source": [
    "#### 특징:\n",
    "1. Gradient-based One-Side Sampling (GOSS)과 Exclusive Feature Bundling (EFB)를 통합한 LightGBM의 간단한 구현입니다.\n",
    "2. 여러 개의 결정 트리를 학습하여 앙상블 방식으로 분류 작업을 수행합니다.\n",
    "3. 각 트리는 데이터의 일부 샘플과 특징을 선택하여 학습됩니다.\n",
    "\n",
    "#### 주요 변수:\n",
    "- `n_estimators`: 학습할 결정 트리의 수\n",
    "- `learning_rate`: 학습률\n",
    "- `max_depth`: 각 트리의 최대 깊이\n",
    "- `min_samples_split`: 노드를 분할하기 위한 최소한의 샘플 수\n",
    "- `subsample`: 데이터의 서브샘플링 비율 (GOSS를 위해 사용됨)\n",
    "- `colsample`: 피처의 서브샘플링 비율 (EFB를 위해 사용됨)\n",
    "- `alpha`: GOSS에서 큰 그라디언트를 가진 데이터의 비율\n",
    "- `beta`: GOSS에서 작은 그라디언트를 가진 데이터의 비율\n",
    "- `trees`: 학습된 결정 트리들을 저장하는 리스트\n",
    "- `bundles`: EFB를 통해 묶인 특징들의 리스트\n",
    "\n",
    "#### 모든 메서드:\n",
    "\n",
    "1. `__init__(...)`: 초기화 함수로, 모델의 하이퍼파라미터와 필요한 변수들을 설정합니다.\n",
    "2. `goss_subsampling(gradients)`: GOSS 알고리즘에 따라 서브샘플링된 데이터 인덱스를 반환합니다.\n",
    "3. `exclusive_feature_bundling(X)`: EFB 알고리즘에 따라 피처를 번들로 묶은 후 번들링된 데이터를 반환합니다.\n",
    "4. `fit(X, y)`: 주어진 학습 데이터 `X`와 레이블 `y`를 사용하여 모델을 학습시킵니다.\n",
    "5. `predict(X)`: 학습된 모델을 사용하여 주어진 데이터 `X`의 레이블을 예측합니다.\n",
    "6. `get_best_split(X, y, feature_subset)`: 주어진 데이터와 피처 집합을 기반으로 최적의 분할을 찾습니다.\n",
    "7. `calculate_gain(y_left, y_right)`: 주어진 왼쪽 및 오른쪽 자식 노드의 목표값을 기반으로 정보 이득을 계산합니다.\n",
    "8. `split_dataset(X, y, feature_index, threshold)`: 주어진 피처와 임계값을 기준으로 데이터셋을 두 부분으로 분할합니다.\n",
    "9. `build_decision_tree(X, y, feature_subset, current_depth=0)`: 주어진 데이터와 피처 집합을 기반으로 결정 트리를 재귀적으로 구성합니다.\n",
    "10. `predict_with_tree(tree, x)`: 주어진 트리와 입력 데이터 포인트를 사용하여 예측값을 계산합니다.\n",
    "\n",
    "#### 동작 방식:\n",
    "1. LightGBM의 구현은 여러 개의 결정 트리를 학습하여 앙상블 방식으로 분류 작업을 수행합니다.\n",
    "2. GOSS는 데이터의 일부만을 사용하여 트리를 학습시키는 데, 큰 그라디언트를 가진 데이터와 일부 작은 그라디언트를 가진 데이터만을 선택합니다.\n",
    "3. EFB는 피처를 번들로 묶어 데이터의 차원을 줄입니다.\n",
    "4. 예측 시, 모든 트리의 예측 결과를 모아서 최종 예측을 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84db188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBM_Implementation:\n",
    "    def __init__(self, n_estimators=5, learning_rate=0.1, max_depth=3, min_samples_split=5, subsample=0.8, colsample=0.8, alpha=0.2, beta=0.2):\n",
    "        # 모델의 초기 설정값을 지정하는 생성자\n",
    "        self.n_estimators = n_estimators  # 생성될 트리의 개수\n",
    "        self.learning_rate = learning_rate  # 각 트리의 학습률\n",
    "        self.max_depth = max_depth  # 트리의 최대 깊이\n",
    "        self.min_samples_split = min_samples_split  # 노드를 분할하는 데 필요한 최소 샘플 수\n",
    "        self.subsample = subsample  # GOSS에서 사용될 데이터 서브샘플링 비율\n",
    "        self.colsample = colsample  # EFB에서 사용될 피처 서브샘플링 비율\n",
    "        self.alpha = alpha  # GOSS에서 큰 그라디언트를 가진 데이터의 선택 비율\n",
    "        self.beta = beta  # GOSS에서 작은 그라디언트를 가진 데이터의 선택 비율\n",
    "        self.trees = []  # 학습된 트리들을 저장할 리스트\n",
    "        self.bundles = []  # EFB를 통해 묶인 피처들의 리스트\n",
    "\n",
    "    def goss_subsampling(self, gradients):\n",
    "        \"\"\"GOSS 알고리즘을 사용하여 데이터의 서브샘플을 선택하는 함수\"\"\"\n",
    "        num_large = int(len(gradients) * self.alpha)  # 큰 그라디언트를 가진 데이터의 개수\n",
    "        large_indices = np.argsort(-np.abs(gradients))[:num_large]  # 큰 그라디언트를 가진 데이터의 인덱스\n",
    "        remaining_indices = np.delete(np.arange(len(gradients)), large_indices)  # 큰 그라디언트를 가진 데이터를 제외한 나머지 인덱스\n",
    "        num_small = int(len(gradients) * self.beta)  # 작은 그라디언트를 가진 데이터의 개수\n",
    "        small_indices = np.random.choice(remaining_indices, num_small, replace=False)  # 작은 그라디언트를 가진 데이터의 인덱스 선택\n",
    "        return np.concatenate([large_indices, small_indices])  # 선택된 데이터의 인덱스 반환\n",
    "\n",
    "    def exclusive_feature_bundling(self, X):\n",
    "        \"\"\"EFB 알고리즘을 사용하여 피처들을 번들로 묶는 함수\"\"\"\n",
    "        n_features = X.shape[1]  # 전체 피처의 개수\n",
    "        is_pair_exclusive = np.ones((n_features, n_features), dtype=bool)  # 피처 쌍이 배타적인지 확인하는 행렬\n",
    "\n",
    "        # 모든 피처 쌍에 대해 배타적인지 확인\n",
    "        for i, j in itertools.combinations(range(n_features), 2):\n",
    "            if np.any(X[:, i] * X[:, j] != 0):\n",
    "                is_pair_exclusive[i, j] = is_pair_exclusive[j, i] = False\n",
    "\n",
    "        visited = set()  # 이미 번들에 추가된 피처의 인덱스를 저장할 집합\n",
    "        bundles = []  # 피처 번들을 저장할 리스트\n",
    "\n",
    "        # 모든 피처에 대해 번들 생성\n",
    "        for i in range(n_features):\n",
    "            if i in visited:\n",
    "                continue\n",
    "            bundle = [i]  # 새로운 번들 생성\n",
    "            visited.add(i)\n",
    "            for j in range(i+1, n_features):\n",
    "                if is_pair_exclusive[i, j] and j not in visited:\n",
    "                    bundle.append(j)  # 번들에 피처 추가\n",
    "                    visited.add(j)\n",
    "            bundles.append(bundle)\n",
    "\n",
    "        # 번들링된 데이터 생성\n",
    "        X_bundled = np.zeros((X.shape[0], len(bundles)))\n",
    "        for idx, bundle in enumerate(bundles):\n",
    "            X_bundled[:, idx] = np.sum(X[:, bundle], axis=1)\n",
    "\n",
    "        return X_bundled, bundles  # 번들링된 데이터와 번들 정보 반환\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"모델을 학습하는 함수\"\"\"\n",
    "        self.prediction = np.full(y.shape, np.mean(y))  # 초기 예측값 설정\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - self.prediction  # 잔차 계산\n",
    "            subsample_indices = self.goss_subsampling(residuals)  # GOSS를 사용하여 서브샘플 선택\n",
    "            X_subset = X[subsample_indices]  # 선택된 서브샘플\n",
    "            y_subset = y[subsample_indices]  # 선택된 서브샘플의 레이블\n",
    "            X_bundle, self.bundles = self.exclusive_feature_bundling(X_subset)  # EFB를 사용하여 피처 번들링\n",
    "            feature_subset = np.random.choice(X_bundle.shape[1], int(X_bundle.shape[1] * self.colsample), replace=False)  # 피처 서브샘플링\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"학습된 모델을 사용하여 주어진 입력 데이터 X에 대한 예측을 수행하는 함수.\"\"\"\n",
    "        final_predictions = np.full(X.shape[0], np.mean(self.prediction))  # 초기 예측값 설정\n",
    "        for tree in self.trees:\n",
    "            tree_predictions = np.array([self.predict_with_tree(tree, x) for x in X])  # 각 트리를 사용한 예측값 계산\n",
    "            final_predictions += self.learning_rate * tree_predictions  # 예측값 업데이트\n",
    "        return (final_predictions > 0.5).astype(int)  # 최종 예측값을 이진 분류 결과로 변환\n",
    "    \n",
    "    def get_best_split(self, X, y, feature_subset):\n",
    "        \"\"\"주어진 데이터와 피처 집합을 기반으로 최적의 분할을 찾는 함수.\"\"\"\n",
    "        best_gain = float('-inf')\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        for feature_index in feature_subset:\n",
    "            unique_values = np.unique(X[:, feature_index])\n",
    "            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
    "                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:\n",
    "                    continue\n",
    "                gain = self.calculate_gain(y_left, y_right)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def calculate_gain(self, y_left, y_right):\n",
    "        \"\"\"주어진 왼쪽 및 오른쪽 자식 노드의 목표값을 기반으로 정보 이득을 계산하는 함수.\"\"\"\n",
    "        def squared_loss_grad(y):\n",
    "            return np.mean(y)\n",
    "        def hessian(y):\n",
    "            return len(y)\n",
    "        g_left, g_right = squared_loss_grad(y_left), squared_loss_grad(y_right)\n",
    "        h_left, h_right = hessian(y_left), hessian(y_right)\n",
    "        gain = 0.5 * (g_left**2 / (h_left + 1e-3) + g_right**2 / (h_right + 1e-3) - (g_left + g_right)**2 / (h_left + h_right + 1e-3))\n",
    "        return gain\n",
    "\n",
    "    def split_dataset(self, X, y, feature_index, threshold):\n",
    "        \"\"\"주어진 피처와 임계값을 기준으로 데이터셋을 두 부분으로 분할하는 함수.\"\"\"\n",
    "        left_mask = X[:, feature_index] < threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def build_decision_tree(self, X, y, feature_subset, current_depth=0):\n",
    "        \"\"\"주어진 데이터와 피처 집합을 기반으로 결정 트리를 재귀적으로 구성하는 함수.\"\"\"\n",
    "        if current_depth >= self.max_depth or len(y) <= self.min_samples_split:\n",
    "            return {'prediction': np.mean(y)}\n",
    "        feature_index, threshold = self.get_best_split(X, y, feature_subset)\n",
    "        if feature_index is None:\n",
    "            return {'prediction': np.mean(y)}\n",
    "        X_left, y_left, X_right, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
    "        return {\n",
    "            'feature_index': feature_index,\n",
    "            'threshold': threshold,\n",
    "            'left': self.build_decision_tree(X_left, y_left, feature_subset, current_depth + 1),\n",
    "            'right': self.build_decision_tree(X_right, y_right, feature_subset, current_depth + 1)\n",
    "        }\n",
    "\n",
    "    def predict_with_tree(self, tree, x):\n",
    "        \"\"\"주어진 트리와 입력 데이터 포인트를 사용하여 예측값을 계산하는 함수.\n",
    "        이 함수는 재귀적으로 호출되어 주어진 입력 데이터 포인트를 트리의 잎 노드까지 전달하고,\n",
    "        해당 노드에서의 예측값을 반환한다.\"\"\"\n",
    "        if 'prediction' in tree:\n",
    "            return tree['prediction']\n",
    "        feature_index = tree['feature_index']\n",
    "        threshold = tree['threshold']\n",
    "        if x[feature_index] < threshold:\n",
    "            return self.predict_with_tree(tree['left'], x)\n",
    "        else:\n",
    "            return self.predict_with_tree(tree['right'], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bed26afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "lgbm_implemantation = LightGBM_Implementation(n_estimators=5, learning_rate=0.1, max_depth=3, min_samples_split=5, subsample=0.8, colsample=0.8)\n",
    "\n",
    "start_time_train = time.time()\n",
    "lgbm_implemantation.fit(X_train.values, y_train.values)\n",
    "end_time_train = time.time()\n",
    "lgbm_implemantation_training_time = end_time_train - start_time_train\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "lgbm_implemantation_predictions = lgbm_implemantation.predict(X_test.values)\n",
    "\n",
    "# 정확도 계산\n",
    "lgbm_implemantation_accuracy = accuracy_score(y_test.values, lgbm_implemantation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dca5191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm_implemantation_accuracy: 0.8395853899308984\n",
      "lgbm_implemantation_training_time: 0.016425609588623047\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('lgbm_implemantation_accuracy:',lgbm_implemantation_accuracy)\n",
    "print('lgbm_implemantation_training_time:',lgbm_implemantation_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61553c",
   "metadata": {},
   "source": [
    "### 3-2-b. 라이브러리 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f0d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 예측\n",
    "scikitlearn_lgbm_model = LGBMClassifier(n_estimators=5, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "start_time_train_lgbm = time.time()  # 학습 시작 시간 저장\n",
    "scikitlearn_lgbm_model.fit(X_train.values, y_train.values)  # 모델 학습\n",
    "end_time_train_lgbm = time.time()  # 학습 종료 시간 저장\n",
    "scikitlearn_lgbm_training_time = end_time_train_lgbm - start_time_train_lgbm\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "scikitlearn_lgbm_predictions = scikitlearn_lgbm_model.predict(X_test.values)\n",
    "\n",
    "# 분류 정확도 계산\n",
    "scikitlearn_lgbm_accuracy = accuracy_score(y_test.values, scikitlearn_lgbm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d065b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikitlearn_lgbm_accuracy: 0.851431391905232\n",
      "scikitlearn_lgbm_training_time: 0.018869400024414062\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('scikitlearn_lgbm_accuracy:',scikitlearn_lgbm_accuracy)\n",
    "print('scikitlearn_lgbm_training_time:',scikitlearn_lgbm_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc351a5",
   "metadata": {},
   "source": [
    "## 3-3. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a10fc",
   "metadata": {},
   "source": [
    "### 3-3-b. 라이브러리 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee7209a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "scikitlearn_catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, verbose=0)\n",
    "\n",
    "start_time_train_catboost = time.time()  # 학습 시작 시간 저장\n",
    "scikitlearn_catboost_model.fit(X_train.values, y_train.values)  # 모델 학습\n",
    "end_time_train_catboost = time.time()  # 학습 종료 시간 저장\n",
    "scikitlearn_catboost_training_time = end_time_train_catboost - start_time_train_catboost\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "scikitlearn_catboost_predictions = scikitlearn_catboost_model.predict(X_test.values)\n",
    "\n",
    "# 분류 정확도 계산\n",
    "scikitlearn_catboost_accuracy = accuracy_score(y_test.values, scikitlearn_catboost_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37129364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikitlearn_catboost_accuracy: 0.9639684106614018\n",
      "scikitlearn_catboost_training_time: 0.40671229362487793\n"
     ]
    }
   ],
   "source": [
    "# 분류 정확도 계산 및 시간\n",
    "print('scikitlearn_catboost_accuracy:',scikitlearn_catboost_accuracy)\n",
    "print('scikitlearn_catboost_training_time:',scikitlearn_catboost_training_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
