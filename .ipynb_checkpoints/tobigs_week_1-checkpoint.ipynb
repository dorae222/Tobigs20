{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d161920a",
   "metadata": {},
   "source": [
    "### 0. 데이터 적재 및 변수 타입 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f54e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786aa011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 열 출력\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86789812",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'WA_Fn-UseC_-HR-Employee-Attrition.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20284\\1071980687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'WA_Fn-UseC_-HR-Employee-Attrition.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WA_Fn-UseC_-HR-Employee-Attrition.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397fe43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c0053",
   "metadata": {},
   "source": [
    "- <I>수치형 변수처럼 보이지만 범주형 변수인 경우가 있다<I>\n",
    "    - 예를 들어, Education은 1~5이지만 이는 교육 수준을 말하며 학사, 석사, 박사처럼 범주형에 해당된다.\n",
    "    - 이에 따라, 수치형 변수들에서도 범주형 변수가 있는지 확인하고 작업해야한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076a544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print('<',i,'>')\n",
    "    print(df[i].unique()[:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e666376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Dataset 소개에 따라 수치형 변수의 범주화 진행\n",
    "\n",
    "df[\"Education\"] = df[\"Education\"].replace({1:\"Below College\",2:\"College\",3:\"Bachelor\",4:\"Master\",5:\"Doctor\"})\n",
    "df[\"EnvironmentSatisfaction\"] = df[\"EnvironmentSatisfaction\"].replace({1:\"Low\",2:\"Medium\",3:\"High\",4:\"Very High\"})\n",
    "df[\"JobInvolvement\"] = df[\"JobInvolvement\"].replace({1:\"Low\",2:\"Medium\",3:\"High\",4:\"Very High\"})\n",
    "df[\"JobLevel\"] = df[\"JobLevel\"].replace({1:\"Entry Level\",2:\"Junior Level\",3:\"Mid Level\",4:\"Senior Level\",\n",
    "                                         5:\"Executive Level\"})\n",
    "df[\"JobSatisfaction\"] = df[\"JobSatisfaction\"].replace({1:\"Low\",2:\"Medium\",3:\"High\",4:\"Very High\"})\n",
    "df[\"PerformanceRating\"] = df[\"PerformanceRating\"].replace({1:\"Low\",2:\"Good\",3:\"Excellent\",4:\"Outstanding\"})\n",
    "df[\"RelationshipSatisfaction\"] = df[\"RelationshipSatisfaction\"].replace({1:\"Low\",2:\"Medium\",3:\"High\",4:\"Very High\"})\n",
    "df[\"WorkLifeBalance\"] = df[\"WorkLifeBalance\"].replace({1:\"Bad\",2:\"Good\",3:\"Better\",4:\"Best\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18706c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3093a17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538c162",
   "metadata": {},
   "source": [
    "### 1. 결측치, 이상치 검토"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d649bf",
   "metadata": {},
   "source": [
    "#### 1-1. 결측치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da47d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 결측치\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print('<',col,'>')\n",
    "    print(df[col].value_counts()[:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29810dd",
   "metadata": {},
   "source": [
    "#### 1-2. 이상치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b230944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 수치형 변수 컬럼\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "constant_cols = [col for col in numeric_cols if df[col].nunique() == 1]\n",
    "variable_numeric_cols = [col for col in numeric_cols if col not in constant_cols]\n",
    "\n",
    "# 기초 통계량\n",
    "df[variable_numeric_cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad811a",
   "metadata": {},
   "source": [
    "##### 1-2-1. IQR 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566487a",
   "metadata": {},
   "source": [
    "- IQR (Interquartile Range)이란?\n",
    "    - IQR 방식은 데이터의 하위 25%와 상위 25% 사이의 범위를 이용하여 이상치를 감지하고 처리하는 방법입니다.<br>IQR은 다음과 같은 수식으로 계산됩니다:\n",
    "\n",
    "$$IQR = Q3 - Q1$$\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/iqr.png\" width=\"500\"/><br>\n",
    "    (이미지 출처: <a href=\"https://en.wikipedia.org/wiki/Interquartile_range\">https://en.wikipedia.org/wiki/Interquartile_range</a>)\n",
    "</div>\n",
    "\n",
    "여기서:\n",
    "- Q1은 데이터의 1사분위수 (하위 25%)를 나타냅니다.\n",
    "- Q3은 데이터의 3사분위수 (상위 25%)를 나타냅니다.\n",
    "\n",
    "IQR 방식을 이용하면, \\(Q1 - 1.5 \\times IQR\\) 보다 작거나 \\(Q3 + 1.5 \\times IQR\\) 보다 큰 데이터 포인트는 이상치로 간주됩니다. \n",
    "\n",
    "IQR 방식은 데이터의 분포가 대칭적이지 않을 때 특히 유용합니다. 그 이유는 IQR 방식이 데이터의 중앙 부분에 초점을 맞추기 때문입니다. 이 방식을 사용하면, 이상치의 영향을 줄이고 데이터의 중앙 부분을 더 잘 대표하는 모델을 만들 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6bf91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe to apply outlier removal\n",
    "df_iqr = df.copy()\n",
    "\n",
    "# IQR 방식\n",
    "def handle_outliers_iqr(col, dataframe):\n",
    "    Q1 = dataframe[col].quantile(0.25)\n",
    "    Q3 = dataframe[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = dataframe[(dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)]\n",
    "    outlier_count = outliers.shape[0]\n",
    "\n",
    "    # Replace outliers with the lower and upper bounds\n",
    "    dataframe.loc[(dataframe[col] < lower_bound), col] = lower_bound\n",
    "    dataframe.loc[(dataframe[col] > upper_bound), col] = upper_bound\n",
    "\n",
    "    return outlier_count\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(len(variable_numeric_cols), 2, figsize=(14, len(variable_numeric_cols)*4))\n",
    "\n",
    "for idx, col in enumerate(variable_numeric_cols):\n",
    "    # 본 데이터\n",
    "    sns.boxplot(x=df[col], ax=axes[idx, 0], color='blue')\n",
    "    axes[idx, 0].set_title(f\"Original {col} (Outliers: {handle_outliers_iqr(col, df_iqr)})\", \n",
    "                            fontsize=14, pad=20)\n",
    "\n",
    "    # 이상치 처리 후 데이터\n",
    "    sns.boxplot(x=df_iqr[col], ax=axes[idx, 1], color='green')\n",
    "    axes[idx, 1].set_title(f\"After IQR Outlier Handling {col}\", \n",
    "                            fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d808ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR 데이터 DF화\n",
    "for col in variable_numeric_cols:\n",
    "    handle_outliers_iqr(col, df_iqr)\n",
    "\n",
    "outlier_indices = df_iqr[df_iqr.isnull().any(axis=1)].index\n",
    "\n",
    "# 종속 변수가'Yes'인 경우\n",
    "attrition_yes_outliers = df.loc[outlier_indices, 'Attrition'] == 'Yes'\n",
    "attrition_yes_outliers_proportion = attrition_yes_outliers.mean()\n",
    "\n",
    "attrition_yes_outliers_proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd750a9",
   "metadata": {},
   "source": [
    "- 종속 변수가'Yes'인 경우가 없어, IQR 방식으로 추출된 이상치를 제거해도 괜찮을 것 같다<br>하지만 추가적으로 다른 방법도 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c653f",
   "metadata": {},
   "source": [
    "##### 1-2-2.  표준점수로 변환 후 -3이하 및 +3 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bad97e",
   "metadata": {},
   "source": [
    "- Z-점수 표준화란?\n",
    "\n",
    "    - Z-점수는 데이터 포인트가 평균으로부터 표준편차의 몇 배만큼 떨어져 있는지를 나타내는 표준화 방법입니다. Z-점수는 다음과 같은 수식으로 계산됩니다:\n",
    "\n",
    "$$Z = \\frac{(X - μ)}{σ}$$\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/z_score.png\" width=\"500\"/><br>\n",
    "    (이미지 출처: <a href=\"https://vitalflux.com/z-score-z-statistics-concepts-formula-examples/\">https://vitalflux.com/z-score-z-statistics-concepts-formula-examples/</a>)\n",
    "</div>\n",
    "\n",
    "여기서:\n",
    "- X는 각 데이터 포인트를 나타냅니다.\n",
    "- μ는 데이터의 평균을 나타냅니다.\n",
    "- σ는 데이터의 표준편차를 나타냅니다.\n",
    "\n",
    "Z-점수는 데이터를 표준화하여 각 데이터 포인트가 원래 분포의 평균으로부터 얼마나 떨어져 있는지를 측정합니다. \n",
    "\n",
    "데이터 포인트의 Z-점수가 3 또는 -3을 초과하는 경우, 이 데이터 포인트는 이상치로 간주됩니다. 이 값은 일반적으로 사용되는 임계값이지만, 필요에 따라 다른 값으로 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fa2a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Create a copy of the dataframe to apply outlier removal\n",
    "df_zscore = df.copy()\n",
    "\n",
    "# z-score +3, -3 함수\n",
    "def handle_outliers_zscore(col, dataframe):\n",
    "    z_scores = zscore(dataframe[col])\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "\n",
    "    outliers = dataframe[abs_z_scores > 3]\n",
    "    outlier_count = outliers.shape[0]\n",
    "\n",
    "    # Replace outliers with NaN\n",
    "    dataframe.loc[(abs_z_scores > 3), col] = np.nan\n",
    "\n",
    "    return outlier_count\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(len(variable_numeric_cols), 2, figsize=(14, len(variable_numeric_cols)*4))\n",
    "\n",
    "for idx, col in enumerate(variable_numeric_cols):\n",
    "    # 본 데이터\n",
    "    sns.distplot(df[col], ax=axes[idx, 0], color='blue', hist=False, kde=True)\n",
    "    axes[idx, 0].set_title(f\"Original {col} (Outliers: {handle_outliers_zscore(col, df_zscore)})\", \n",
    "                            fontsize=14, pad=20)\n",
    "\n",
    "    # 이상치 처리 후 데이터\n",
    "    sns.distplot(df_zscore[col], ax=axes[idx, 1], color='green', hist=False, kde=True)\n",
    "    axes[idx, 1].set_title(f\"After Z-Score Outlier Handling {col}\", \n",
    "                            fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4ba1f",
   "metadata": {},
   "source": [
    "- 유의미한 변화가 보이지 않아, 다른 방식으로 추가 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be216ad",
   "metadata": {},
   "source": [
    "##### 1-2-3.  표준화 후 PCA와 DBSCAN 알고리즘으로 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7246bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 컬럼만 선택\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(numeric_columns)\n",
    "\n",
    "# 수치형 컬럼만 데이터 프레임화\n",
    "numeric_scaled = pd.DataFrame(numeric_scaled, columns=numeric_columns.columns)\n",
    "\n",
    "numeric_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71309813",
   "metadata": {},
   "source": [
    "- DBSCAN이란?\n",
    "    - 데이터의 밀도에 기반한 클러스터링 알고리즘입니다.<br>데이터 포인트의 밀도를 계산하여,<br>데이터 포인트의 밀도가 일정 수준 이상이면 그 데이터 포인트를 클러스터의 일부로 간주합니다.\n",
    "\n",
    "- DBSCAN은 다음과 같은 주요 특징\n",
    "\n",
    "    - 클러스터의 수를 미리 정의할 필요가 없습니다: DBSCAN은 데이터의 분포에 따라 클러스터의 수를 자동으로 결정합니다.\n",
    "    - 임의의 형태의 클러스터를 찾을 수 있습니다: DBSCAN은 k-means와 같은 알고리즘과 달리 원형의 클러스터만 찾지 않고, 임의의 형태의 클러스터를 찾을 수 있습니다.\n",
    "    - 이상치를 처리할 수 있습니다: DBSCAN은 밀도가 낮은 영역의 데이터 포인트를 이상치로 간주하여, 이상치 탐지에도 사용할 수 있습니다.\n",
    "\n",
    "- DBSCAN은 다음과 같은 두 개의 주요 매개변수\n",
    "\n",
    "    - `eps`: 이 매개변수는 클러스터의 최대 반경을 정의합니다. `eps` 거리 내에 있는 데이터 포인트의 수가 `min_samples` 이상이면, 그 데이터 포인트를 클러스터의 일부로 간주합니다.\n",
    "    - `min_samples`: 클러스터를 형성하는 데 필요한 최소 데이터 포인트의 수를 정의합니다.\n",
    "\n",
    "이러한 특징 때문에 DBSCAN은 공간적 클러스터링과 이상치 탐지에 효과적인 알고리즘이라고 할 수 있습니다.<br><br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/dbscan.png\" width=\"500\"/><br>\n",
    "    (이미지 출처: <a href=\"https://yganalyst.github.io/ml/ML_clustering/\">https://yganalyst.github.io/ml/ML_clustering/</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=3.0, min_samples=5)\n",
    "clusters = dbscan.fit_predict(numeric_scaled)\n",
    "\n",
    "# 이상치 수\n",
    "outliers = (clusters == -1).sum() # (clusters == -1) -1일 경우, 클러스터에 속하지 않음\n",
    "\n",
    "outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d551afd1",
   "metadata": {},
   "source": [
    "- PCA (Principal Component Analysis)란?\n",
    "    - 즉 주성분 분석은 고차원 데이터를 저차원 데이터로 변환하는 데 사용되는 통계적 방법입니다.<br>PCA는 데이터의 분산이 최대가 되는 방향으로 축을 회전시키며, 이렇게 생성된 새로운 축을 주성분이라고 합니다.<br>이 주성분들은 원래의 특성들의 선형 조합으로 이루어져 있습니다.\n",
    "\n",
    "- PCA 주요 특징\n",
    "\n",
    "    - 데이터의 차원 축소: PCA는 고차원의 데이터를 저차원의 데이터로 축소시키는 데 주로 사용됩니다. 이는 데이터를 시각화하거나, 머신러닝 모델의 성능을 향상시키는 데 도움이 될 수 있습니다.\n",
    "    - 정보 손실 최소화: PCA는 원본 데이터의 분산을 최대한 보존하려고 합니다. 이는 원본 데이터의 정보를 최대한 보존하면서 차원을 축소하는 데 도움이 됩니다.\n",
    "    - 상관관계 감소: PCA는 변환된 데이터의 특성들이 서로 직교하도록 만듭니다. 이는 변환된 특성들 사이의 상관관계를 제거합니다.\n",
    "\n",
    "PCA는 이러한 특징 때문에 데이터 전처리, 시각화, 특성 추출 등 다양한 분야에서 널리 사용됩니다.\n",
    "\n",
    "- PCA의 축(차원)\n",
    "    - 원본 데이터의 분산이 가장 큰 방향을 나타내는 벡터입니다.<br>첫 번째 주성분은 데이터의 분산이 가장 큰 방향을 찾아내고, 두 번째 주성분은 그 다음으로 분산이 큰 방향을 찾아냅니다.<br>이 과정은 원하는 차원의 수만큼 계속되며, 각 주성분은 이전의 주성분들과 직교(orthogonal)하게 설정됩니다.<br><br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/pca.png\" width=\"500\"/><br>\n",
    "    (이미지 출처: <a href=\"https://m.blog.naver.com/sanghan1990/221156213790\">https://m.blog.naver.com/sanghan1990/221156213790</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a8617",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2차원으로 데이터 축소\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(numeric_scaled)\n",
    "\n",
    "# Plot the reduced data and highlight the outliers\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# 이상치인 경우\n",
    "plt.scatter(reduced_data[clusters != -1, 0], reduced_data[clusters != -1, 1], \n",
    "            c=clusters[clusters != -1], cmap='Paired', label='Outlier X')\n",
    "\n",
    "# 이상치 아닌 경우\n",
    "plt.scatter(reduced_data[clusters == -1, 0], reduced_data[clusters == -1, 1], \n",
    "            color='red', label='Outlier O')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Outlier detection using DBSCAN')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eabbd60b",
   "metadata": {},
   "source": [
    "- 시각화된 결과물을 봤을 때<br>대체로 좌하단 방향으로 몰려있으며, 우상단에 가까운 데이터는 이상치로 파악됨을 알 수 있음\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/outlier_detection_dbscan.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835801e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DBSCAN 알고리즘을 이용하여 파악된 이상치 제거하기\n",
    "\n",
    "# 이상치에 대한 boolean mask 생성\n",
    "outlier_mask = (clusters == -1)\n",
    "\n",
    "# 이상치 데이터 추출\n",
    "data_without_outliers = numeric_scaled[~outlier_mask]\n",
    "\n",
    "data_without_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e01c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('제거된 데이터수: ',len(df)-len(data_without_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b383e",
   "metadata": {},
   "source": [
    "##### 1-2-4.  KMeans 클러스터링 알고리즘으로 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45473b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN 진행전 스케일링한 수치형 변수 데이터 프레임 사용\n",
    "# numeric_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981e0e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f02681",
   "metadata": {},
   "source": [
    "### 2. 유의미한 시각화 5개 이상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144488de",
   "metadata": {},
   "source": [
    "#### 2-1. 이탈 여부에 따른 직무 만족도 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48370b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aeccf76",
   "metadata": {},
   "source": [
    "#### 2-2. 이탈 여부에 따른 급여 분포 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb7587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad8dcd3",
   "metadata": {},
   "source": [
    "#### 2-3. 직급에 따른 이탈 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bc31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b16b3871",
   "metadata": {},
   "source": [
    "#### 2-4. 직무 스트레스 수준과 이탈 간의 상관 관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b829fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be7c13bd",
   "metadata": {},
   "source": [
    "#### 2-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363f2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9ff1c3e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d05422",
   "metadata": {},
   "source": [
    "### 3. 수치형 변수 간 상관관계 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2abe8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "df_corr = df.drop(columns=['EmployeeCount','StandardHours'])\n",
    "corr_matrix = df_corr.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c4d64",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259387d",
   "metadata": {},
   "source": [
    "### 4. 파생변수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff089a",
   "metadata": {},
   "source": [
    "- 가상 데이터이지만 한국이라는 가정하에 국내 퇴사 원인 설문조사를 근거로 파생변수 새롭게 생성\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/Attrition_caustion.png\" width=\"500\"/><br>\n",
    "    (이미지 출처: <a href=\"https://m.blog.naver.com/sanghan1990/221156213790\">https://m.blog.naver.com/sanghan1990/221156213790</a>)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfd8c0",
   "metadata": {},
   "source": [
    "#### 스트레스 지수\n",
    "- 직무 만족도, 근무 조건 등을 고려하여 계산"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
